{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNN2E2qMp2yONA0+5bG2iDD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsusong/21-study-tensorflow/blob/main/%EB%8D%94%EB%AF%B8%20%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A5%BC%20%ED%99%9C%EC%9A%A9%ED%95%9C%20%EA%B0%90%EC%A0%95%20%EB%B6%84%EC%84%9D%20%EB%AA%A8%EB%8D%B8%EB%A7%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_O0zwVtPDOy"
      },
      "source": [
        "# 텐서플로2와 머신러닝으로 시작하는 자연어 처리 \n",
        "## 로지스틱 회귀부터 BERT와 GPT2 까지\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWVG4MK3S73M"
      },
      "source": [
        "더미 데이터를 활용한 감정 분석 모델링"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDNypLqphYc"
      },
      "source": [
        "- 모델 학습에는 케라스 모델의 내장 API 활용\n",
        "\n",
        "- 모델은 심층 신경망 DNN 구조를 사용해 텍스트의 긍정/부정을 예측하는 감정 분석 모델\n",
        "\n",
        "1. 각 단어로 구성된 입력값은 임베딩된 벡터로 변형된다. \n",
        "2. 각 벡터를 평균해서 하나의 벡터로 만든다. \n",
        "3. 하나의 은닉층을 거친 후 하나의 결과값을 뽑는 구조 \n",
        "4. 결과값에 시그모이드 함수를 적용해 0과 1 사이의 값을 구함 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eTsHrknqD4G"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "samples = ['너 오늘 이뻐 보인다',\n",
        "           '나는 오늘 기분이 더러워',\n",
        "           '끝내주는데, 좋은 일이 있나봐',\n",
        "           '나 좋은 일이 생겼어',\n",
        "           '아 오늘 진짜 짜증나',\n",
        "           '환상적인데, 정말 좋은거 같아'\n",
        "           ]\n",
        "\n",
        "labels = [[1],[0],[1],[1],[0],[1]]\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(samples)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEFw_uAuqoYf"
      },
      "source": [
        "- 모델 구축 및 모델 학습에 필요한 변수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN8k7wXhqyd-"
      },
      "source": [
        "batch_size = 2\n",
        "num_epochs = 100\n",
        "vocab_size = len(word_index) +1\n",
        "emb_size = 128\n",
        "hidden_dimension = 256\n",
        "output_dimension = 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hna-dHnwq7qI"
      },
      "source": [
        "- 학습 과정에서 적용할 배치 사이즈와 에폭 수, 모델의 하이퍼파라미터에 해당하는 여러 차원의 크기(임베딩 층,은닉 층, 출력 층)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvxBWClLrG8Y"
      },
      "source": [
        "- Sequential API를 활용해 심층 신경망 모델을 생성하려면 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHBjqIoErR2E"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    layers.Embedding(vocab_size, emb_size, input_length =4),\n",
        "    layers.Lambda(lambda x : tf.reduce_mean(x,axis =1)),\n",
        "    layers.Dense(hidden_dimension, activation='relu'),\n",
        "    layers.Dense(output_dimension, activation='sigmoid')])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6kcyEfvrrsX"
      },
      "source": [
        "1. 입력값을 임베딩하는 Embedding 층을 모델에 추가 \n",
        "\n",
        "2. 임베딩된 단어의 벡터를 평균하기 위해 람다 층을 사용 : 람다 층은 텐서플로 연산을 Sequential API와 Functional API에 적용하기 위해 사용하는 방법\n",
        "\n",
        "3. 람다 층을 활용해 평균을 낸 후 하나의 은닉층을 통과한 후 최종 출력값을 뽑기 위해 두개의 Dense 층을 모델에 추가\n",
        "\n",
        "4. 최종 출력값을 뽑은 Dense 층의 경우 0과 1 사이의 확률값을 뽑기 위해 활성화 함수를 시그모이드 함수로 정의\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYm1tmQBtfb4"
      },
      "source": [
        " - 구성된 모델 학습하기 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCDFPlDptxWk"
      },
      "source": [
        "- 학습 과정 정의 하기 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrFyVKhXt3EN"
      },
      "source": [
        "- 옵티마이저의 경우 : 아담 최적화 알고리즘 사용\n",
        "\n",
        "- 학습 : 이진 분류 문제이므로 이진 교차 엔트로피 손실 함수 사용 \n",
        "\n",
        "- 모델 성능 측정을 위한 기준 평가 지표 : 정확도를 평가 지표 정의 \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev2LIXYVuJDb"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYcj2ijiuQ_f"
      },
      "source": [
        "학습 진행 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfAOmFiqucYG"
      },
      "source": [
        "model.fit(sequences, labels, epochs=num_epochs, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_52RV0Tsuln4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp666UJCT-8Q"
      },
      "source": [
        "# Functional API 방법으로 동일한 모델 구현 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkHojsQiUIGP"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "inputs = layers.Input(shape=(4,))\n",
        "embed_output = layers.Embedding(vocab_size, emb_size)(inputs)\n",
        "pooled_output = tf.reduce_mean(embed_output, axis=1)\n",
        "hidden_layer = layers.Dense(hidden_dimension, activation='relu')(pooled_output)\n",
        "outputs = layers.Dense(output_dimension, activation='sigmoid')(hidden_layer)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              loss='binary_crossentorpy',\n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "model.fit(input_sequences, labels, epochs=num_epochs, batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHJaTD9KVxPx"
      },
      "source": [
        "# Subclassing 방법으로 동일한 모델 구현 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOh4f06HUTvX"
      },
      "source": [
        "class CustomModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dimension, hidden_dimension, output_dimension):\n",
        "        super(CustomModel, self).__init__(name='my_model')\n",
        "        self.embedding = layers.Embedding(vocab_size, embed_dimension)\n",
        "        self.dense_layer = layers.Dense(hidden_dimension, activation='relu')\n",
        "        self.output_layer = layers.Dense(output_dimension, activation='sigmoid')\n",
        "\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x = tf.reduce_mean(x, axis=1)\n",
        "        x = self.dense_layer(x)\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = CustomModel(vocab_size = vocab_size,\n",
        "                    embed_dimension=emb_size,\n",
        "                    hidden_dimension=hidden_dimension,\n",
        "                    output_dimension=output_dimension\n",
        "                    )\n",
        "\n",
        "model.compile(optimizer= tf.keras.optimizers.Adam(0.001),\n",
        "              loss='binary_crossentorpy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(input_sequences, labels, epochs=num_epochs, batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxMMiwUsWuPP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}